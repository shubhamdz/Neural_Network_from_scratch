{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b52121",
   "metadata": {},
   "source": [
    " #### Backpropagation through a single neuron (chcek book NNFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85b8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss :36.0\n",
      "Iteration 2, Loss :33.872399999999985\n",
      "Iteration 3, Loss :31.870541159999995\n",
      "Iteration 4, Loss :29.98699217744401\n",
      "Iteration 5, Loss :28.21476093975706\n",
      "Iteration 6, Loss :26.54726856821742\n",
      "Iteration 7, Loss :24.978324995835766\n",
      "Iteration 8, Loss :23.502105988581878\n",
      "Iteration 9, Loss :22.113131524656684\n",
      "Iteration 10, Loss :20.80624545154949\n",
      "Iteration 11, Loss :19.576596345362915\n",
      "Iteration 12, Loss :18.419619501351963\n",
      "Iteration 13, Loss :17.331019988822064\n",
      "Iteration 14, Loss :16.306756707482677\n",
      "Iteration 15, Loss :15.343027386070442\n",
      "Iteration 16, Loss :14.43625446755368\n",
      "Iteration 17, Loss :13.583071828521266\n",
      "Iteration 18, Loss :12.780312283455652\n",
      "Iteration 19, Loss :12.024995827503426\n",
      "Iteration 20, Loss :11.314318574097976\n",
      "Iteration 21, Loss :10.645642346368787\n",
      "Iteration 22, Loss :10.016484883698395\n",
      "Iteration 23, Loss :9.424510627071816\n",
      "Iteration 24, Loss :8.867522049011871\n",
      "Iteration 25, Loss :8.34345149591527\n",
      "Iteration 26, Loss :7.850353512506679\n",
      "Iteration 27, Loss :7.386397619917536\n",
      "Iteration 28, Loss :6.949861520580408\n",
      "Iteration 29, Loss :6.539124704714106\n",
      "Iteration 30, Loss :6.152662434665503\n",
      "Iteration 31, Loss :5.7890400847767705\n",
      "Iteration 32, Loss :5.446907815766464\n",
      "Iteration 33, Loss :5.124995563854671\n",
      "Iteration 34, Loss :4.8221083260308575\n",
      "Iteration 35, Loss :4.537121723962434\n",
      "Iteration 36, Loss :4.268977830076255\n",
      "Iteration 37, Loss :4.016681240318748\n",
      "Iteration 38, Loss :3.7792953790159096\n",
      "Iteration 39, Loss :3.5559390221160707\n",
      "Iteration 40, Loss :3.345783025909011\n",
      "Iteration 41, Loss :3.148047249077789\n",
      "Iteration 42, Loss :2.9619976566572896\n",
      "Iteration 43, Loss :2.786943595148845\n",
      "Iteration 44, Loss :2.622235228675549\n",
      "Iteration 45, Loss :2.4672611266608238\n",
      "Iteration 46, Loss :2.321445994075166\n",
      "Iteration 47, Loss :2.1842485358253256\n",
      "Iteration 48, Loss :2.0551594473580463\n",
      "Iteration 49, Loss :1.9336995240191863\n",
      "Iteration 50, Loss :1.8194178821496518\n",
      "Iteration 51, Loss :1.7118902853146067\n",
      "Iteration 52, Loss :1.6107175694525138\n",
      "Iteration 53, Loss :1.515524161097869\n",
      "Iteration 54, Loss :1.4259566831769857\n",
      "Iteration 55, Loss :1.3416826432012259\n",
      "Iteration 56, Loss :1.2623891989880334\n",
      "Iteration 57, Loss :1.18778199732784\n",
      "Iteration 58, Loss :1.1175840812857634\n",
      "Iteration 59, Loss :1.0515348620817766\n",
      "Iteration 60, Loss :0.9893891517327431\n",
      "Iteration 61, Loss :0.930916252865338\n",
      "Iteration 62, Loss :0.8758991023209968\n",
      "Iteration 63, Loss :0.8241334653738256\n",
      "Iteration 64, Loss :0.7754271775702323\n",
      "Iteration 65, Loss :0.7295994313758316\n",
      "Iteration 66, Loss :0.6864801049815187\n",
      "Iteration 67, Loss :0.6459091307771115\n",
      "Iteration 68, Loss :0.6077359011481847\n",
      "Iteration 69, Loss :0.571818709390327\n",
      "Iteration 70, Loss :0.5380242236653578\n",
      "Iteration 71, Loss :0.5062269920467349\n",
      "Iteration 72, Loss :0.47630897681677353\n",
      "Iteration 73, Loss :0.4481591162869011\n",
      "Iteration 74, Loss :0.4216729125143454\n",
      "Iteration 75, Loss :0.3967520433847474\n",
      "Iteration 76, Loss :0.3733039976207088\n",
      "Iteration 77, Loss :0.35124173136132436\n",
      "Iteration 78, Loss :0.3304833450378702\n",
      "Iteration 79, Loss :0.3109517793461322\n",
      "Iteration 80, Loss :0.29257452918677535\n",
      "Iteration 81, Loss :0.27528337451183676\n",
      "Iteration 82, Loss :0.25901412707818716\n",
      "Iteration 83, Loss :0.24370639216786655\n",
      "Iteration 84, Loss :0.22930334439074554\n",
      "Iteration 85, Loss :0.21575151673725296\n",
      "Iteration 86, Loss :0.2030006020980815\n",
      "Iteration 87, Loss :0.1910032665140846\n",
      "Iteration 88, Loss :0.17971497346310225\n",
      "Iteration 89, Loss :0.16909381853143338\n",
      "Iteration 90, Loss :0.1591003738562249\n",
      "Iteration 91, Loss :0.14969754176132236\n",
      "Iteration 92, Loss :0.14085041704322837\n",
      "Iteration 93, Loss :0.13252615739597357\n",
      "Iteration 94, Loss :0.12469386149387143\n",
      "Iteration 95, Loss :0.11732445427958357\n",
      "Iteration 96, Loss :0.1103905790316602\n",
      "Iteration 97, Loss :0.1038664958108892\n",
      "Iteration 98, Loss :0.09772798590846558\n",
      "Iteration 99, Loss :0.09195226194127534\n",
      "Iteration 100, Loss :0.08651788326054576\n",
      "Iteration 101, Loss :0.08140467635984766\n",
      "Iteration 102, Loss :0.07659365998698062\n",
      "Iteration 103, Loss :0.07206697468175022\n",
      "Iteration 104, Loss :0.06780781647805834\n",
      "Iteration 105, Loss :0.06380037452420508\n",
      "Iteration 106, Loss :0.06002977238982451\n",
      "Iteration 107, Loss :0.056482012841585764\n",
      "Iteration 108, Loss :0.05314392588264784\n",
      "Iteration 109, Loss :0.050003119862983315\n",
      "Iteration 110, Loss :0.04704793547908108\n",
      "Iteration 111, Loss :0.044267402492267266\n",
      "Iteration 112, Loss :0.04165119900497404\n",
      "Iteration 113, Loss :0.03918961314378035\n",
      "Iteration 114, Loss :0.036873507006982977\n",
      "Iteration 115, Loss :0.034694282742870286\n",
      "Iteration 116, Loss :0.032643850632766785\n",
      "Iteration 117, Loss :0.030714599060370322\n",
      "Iteration 118, Loss :0.028899366255902493\n",
      "Iteration 119, Loss :0.027191413710178584\n",
      "Iteration 120, Loss :0.025584401159906914\n",
      "Iteration 121, Loss :0.024072363051356495\n",
      "Iteration 122, Loss :0.02264968639502138\n",
      "Iteration 123, Loss :0.021311089929075627\n",
      "Iteration 124, Loss :0.02005160451426725\n",
      "Iteration 125, Loss :0.018866554687474075\n",
      "Iteration 126, Loss :0.017751541305444478\n",
      "Iteration 127, Loss :0.016702425214292563\n",
      "Iteration 128, Loss :0.015715311884128023\n",
      "Iteration 129, Loss :0.014786536951776086\n",
      "Iteration 130, Loss :0.013912652617925996\n",
      "Iteration 131, Loss :0.013090414848206543\n",
      "Iteration 132, Loss :0.012316771330677616\n",
      "Iteration 133, Loss :0.011588850145034585\n",
      "Iteration 134, Loss :0.010903949101463065\n",
      "Iteration 135, Loss :0.010259525709566468\n",
      "Iteration 136, Loss :0.00965318774013127\n",
      "Iteration 137, Loss :0.009082684344689475\n",
      "Iteration 138, Loss :0.008545897699918217\n",
      "Iteration 139, Loss :0.008040835145853157\n",
      "Iteration 140, Loss :0.007565621788733219\n",
      "Iteration 141, Loss :0.0071184935410191314\n",
      "Iteration 142, Loss :0.0066977905727448606\n",
      "Iteration 143, Loss :0.0063019511498957235\n",
      "Iteration 144, Loss :0.0059295058369368625\n",
      "Iteration 145, Loss :0.005579072041973911\n",
      "Iteration 146, Loss :0.005249348884293189\n",
      "Iteration 147, Loss :0.004939112365231465\n",
      "Iteration 148, Loss :0.004647210824446307\n",
      "Iteration 149, Loss :0.004372560664721486\n",
      "Iteration 150, Loss :0.004114142329436494\n",
      "Iteration 151, Loss :0.003870996517766834\n",
      "Iteration 152, Loss :0.0036422206235667827\n",
      "Iteration 153, Loss :0.003426965384714017\n",
      "Iteration 154, Loss :0.0032244317304774505\n",
      "Iteration 155, Loss :0.0030338678152062068\n",
      "Iteration 156, Loss :0.0028545662273275238\n",
      "Iteration 157, Loss :0.002685861363292443\n",
      "Iteration 158, Loss :0.002527126956721865\n",
      "Iteration 159, Loss :0.0023777737535795864\n",
      "Iteration 160, Loss :0.00223724732474303\n",
      "Iteration 161, Loss :0.0021050260078507234\n",
      "Iteration 162, Loss :0.0019806189707867374\n",
      "Iteration 163, Loss :0.0018635643896132343\n",
      "Iteration 164, Loss :0.0017534277341871227\n",
      "Iteration 165, Loss :0.001649800155096641\n",
      "Iteration 166, Loss :0.0015522969659304577\n",
      "Iteration 167, Loss :0.001460556215243966\n",
      "Iteration 168, Loss :0.0013742373429230384\n",
      "Iteration 169, Loss :0.0012930199159562866\n",
      "Iteration 170, Loss :0.0012166024389232565\n",
      "Iteration 171, Loss :0.0011447012347829176\n",
      "Iteration 172, Loss :0.0010770493918072417\n",
      "Iteration 173, Loss :0.0010133957727514104\n",
      "Iteration 174, Loss :0.0009535040825818078\n",
      "Iteration 175, Loss :0.0008971519913012032\n",
      "Iteration 176, Loss :0.0008441303086153036\n",
      "Iteration 177, Loss :0.0007942422073761319\n",
      "Iteration 178, Loss :0.0007473024929201971\n",
      "Iteration 179, Loss :0.0007031369155886336\n",
      "Iteration 180, Loss :0.0006615815238773228\n",
      "Iteration 181, Loss :0.0006224820558161947\n",
      "Iteration 182, Loss :0.0005856933663174669\n",
      "Iteration 183, Loss :0.0005510788883681015\n",
      "Iteration 184, Loss :0.0005185101260655451\n",
      "Iteration 185, Loss :0.00048786617761505856\n",
      "Iteration 186, Loss :0.00045903328651801555\n",
      "Iteration 187, Loss :0.0004319044192847635\n",
      "Iteration 188, Loss :0.0004063788681050637\n",
      "Iteration 189, Loss :0.0003823618770000461\n",
      "Iteration 190, Loss :0.0003597642900693548\n",
      "Iteration 191, Loss :0.0003385022205262612\n",
      "Iteration 192, Loss :0.00031849673929316324\n",
      "Iteration 193, Loss :0.0002996735820009465\n",
      "Iteration 194, Loss :0.0002819628733046985\n",
      "Iteration 195, Loss :0.0002652988674923804\n",
      "Iteration 196, Loss :0.0002496197044235683\n",
      "Iteration 197, Loss :0.00023486717989212869\n",
      "Iteration 198, Loss :0.00022098652956051694\n",
      "Iteration 199, Loss :0.0002079262256634926\n",
      "Iteration 200, Loss :0.00019563778572677352\n",
      "Final Weights :  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final Bias :  0.6009044964039992\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#inital parameter\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target_output =0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0,1.0,0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "    #forward pass\n",
    "    linear_output = np.dot(weights,inputs)+bias\n",
    "    output = relu(linear_output)\n",
    "    loss = (output - target_output)**2   \n",
    "    #backward pass\n",
    "    dloss_doutput = 2*(output-target_output)\n",
    "    doutput_dlinear = relu_derivative(linear_output)\n",
    "    #we skip dsum_dmul beacause it is always 1 \n",
    "    dlinear_dweights = inputs\n",
    "    dlinear_dbias = 1.0\n",
    "    \n",
    "    dloss_dlinear = dloss_doutput * doutput_dlinear\n",
    "    dloss_dweights = dloss_dlinear*dlinear_dweights\n",
    "    dloss_dbias = dloss_dlinear*dlinear_dbias\n",
    "    \n",
    "    #update weights ans bias \n",
    "    weights -= learning_rate*dloss_dweights \n",
    "    bias -= learning_rate*dloss_dbias\n",
    "    \n",
    "    #print the loss fir this iteration \n",
    "    print(f\"Iteration {iteration + 1}, Loss :{loss}\")\n",
    "\n",
    "print('Final Weights : ', weights)\n",
    "print('Final Bias : ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c862be",
   "metadata": {},
   "source": [
    "#### Backpropagation through a layer of neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3083aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 466.56000000000006\n",
      "Iteration 20, Loss: 5.32959636083938\n",
      "Iteration 40, Loss: 0.41191523404899866\n",
      "Iteration 60, Loss: 0.031836212079467595\n",
      "Iteration 80, Loss: 0.002460565465389601\n",
      "Iteration 100, Loss: 0.000190172825660145\n",
      "Iteration 120, Loss: 1.4698126966451542e-05\n",
      "Iteration 140, Loss: 1.1359926717815175e-06\n",
      "Iteration 160, Loss: 8.779889800154524e-08\n",
      "Iteration 180, Loss: 6.7858241357822796e-09\n",
      "Final weights:\n",
      " [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
      " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
      " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
      "Final biases:\n",
      " [-0.00698895 -0.04024714 -0.06451539]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial inputs\n",
    "inputs = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Initial weights and biases\n",
    "weights = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "\n",
    "biases = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# ReLU activation function and its derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(200):\n",
    "    # Forward pass\n",
    "    z = np.dot(weights, inputs) + biases\n",
    "    a = relu(z)\n",
    "    y = np.sum(a)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = y ** 2\n",
    "\n",
    "    # Backward pass\n",
    "    # Gradient of loss with respect to output y\n",
    "    dL_dy = 2 * y\n",
    "\n",
    "    # Gradient of y with respect to a\n",
    "    dy_da = np.ones_like(a)\n",
    "\n",
    "    # Gradient of loss with respect to a\n",
    "    dL_da = dL_dy * dy_da\n",
    "\n",
    "    # Gradient of a with respect to z (ReLU derivative)\n",
    "    da_dz = relu_derivative(z)\n",
    "\n",
    "    # Gradient of loss with respect to z\n",
    "    dL_dz = dL_da * da_dz\n",
    "\n",
    "    # Gradient of z with respect to weights and biases\n",
    "    dL_dW = np.outer(dL_dz, inputs)\n",
    "    dL_db = dL_dz\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights -= learning_rate * dL_dW\n",
    "    biases -= learning_rate * dL_db\n",
    "\n",
    "    # Print the loss every 20 iterations\n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "\n",
    "# Final weights and biases\n",
    "print(\"Final weights:\\n\", weights)\n",
    "print(\"Final biases:\\n\", biases)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22649d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
